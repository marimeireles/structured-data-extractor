package fr.sparna.rdf.extractor.cli.crawl;

import java.io.File;

import com.beust.jcommander.Parameter;

import edu.uci.ics.crawler4j.crawler.CrawlConfig;

public class ArgumentsCrawlBak {

	@Parameter(
			names = { "-cc", "--crawlConfig" },
			description = "Path to the file containing the crawl configuration",
			required = true
	)
	private File crawlConfig;
	
	@Parameter(
			names = { "-ec", "--extractionConfig" },
			description = "Path to the file containing the extraction configuration",
			required = true
	)
	private File extractConfig;
	
	@Parameter(
			names = { "-S", "--storage" },
			description = "Crawl storage folder.",
			required = false
	)
	private File storage = new File("crawl");
	
	
	@Parameter(
			names = { "-t", "--threads" },
			description = "Number of crawl threads",
			required = false
	)
	private int threads = 2;
	
	@Parameter(
			names = { "-s", "--seeds" },
			description = "File containing seeds, one per line",
			required = true
	)
	private File seeds;
	
	@Parameter(
			names = { "-d", "--decideRules" },
			description = "Path to file containing the decide rules Spring configuration",
			required = false
	)
	private String decideRules = "deciderules.xml";
	
	@Parameter(
			names = { "-o", "--output" },
			description = "Path to either a file that will contain the dump of the extracted data, or URL of the target remote repository",
			required = true
	)
	private String output;
	
	@Parameter(
			names = { "-ec", "--extractContent" },
			description = "Tells the extractor to also extract the content of the webapges, and store it in a dedicated RDF predicate.",
			required = false
	)
	private boolean extractContent = false;

	@Parameter(
			names = { "-ej", "--extractJsonLd" },
			description = "Tells the extractor to extract JSON-LD",
			arity = 1,
			required = false
	)
	private boolean extractJsonLd = true;
	
	@Parameter(
			names = { "-p", "--politeness" },
			description = "The politeness delay to wait between each request, in milliseconds.",
			required = false
	)
	private int politenessDelay = 1000;
	
	@Parameter(
			names = { "-M", "--maximum" },
			description = "The maximum number of pages to fetch during the crawl. -1 equals no limit. Defaults to 1000.",
			required = false
	)
	private int maxNumberOfPagesToFetch = 1000;

	@Parameter(
			names = { "-D", "--depth" },
			description = "The maximum depth of crawling from the seeds. -1 means no limit.",
			required = false
	)
	private int maxDepthOfCrawling = 2;

	public void initCrawlConfig(CrawlConfig config) {
		
		/*
		 * crawlStorageFolder is a folder where intermediate crawl data is
		 * stored.
		 */
		config.setCrawlStorageFolder(this.getStorage().getAbsolutePath());

		/*
		 * Be polite: Make sure that we don't send more than 1 request per
		 * second (1000 milliseconds between requests).
		 */
		config.setPolitenessDelay(this.getPolitenessDelay());

		/*
		 * You can set the maximum crawl depth here. The default value is -1 for
		 * unlimited depth
		 */
		config.setMaxDepthOfCrawling(this.getMaxDepthOfCrawling());

		/*
		 * You can set the maximum number of pages to crawl. The default value
		 * is -1 for unlimited number of pages
		 */
		config.setMaxPagesToFetch(this.getMaxNumberOfPagesToFetch());

		/*
		 * Do you want crawler4j to crawl also binary data ?
		 * example: the contents of pdf, or the metadata of images etc
		 */
		config.setIncludeBinaryContentInCrawling(false);

		/*
		 * Do you need to set a proxy? If so, you can use:
		 * config.setProxyHost("proxyserver.example.com");
		 * config.setProxyPort(8080);
		 *
		 * If your proxy also needs authentication:
		 * config.setProxyUsername(username); config.getProxyPassword(password);
		 */

		/*
		 * This config parameter can be used to set your crawl to be resumable
		 * (meaning that you can resume the crawl from a previously
		 * interrupted/crashed crawl). Note: if you enable resuming feature and
		 * want to start a fresh crawl, you need to delete the contents of
		 * rootFolder manually.
		 */
		config.setResumableCrawling(false);
	}
	
	public File getStorage() {
		return storage;
	}

	public void setStorage(File storage) {
		this.storage = storage;
	}

	public int getThreads() {
		return threads;
	}

	public void setThreads(int threads) {
		this.threads = threads;
	}

	public File getSeeds() {
		return seeds;
	}

	public void setSeeds(File seeds) {
		this.seeds = seeds;
	}

	public String getDecideRules() {
		return decideRules;
	}

	public void setDecideRules(String decideRules) {
		this.decideRules = decideRules;
	}

	public String getOutput() {
		return output;
	}

	public void setOutput(String output) {
		this.output = output;
	}

	public boolean isExtractContent() {
		return extractContent;
	}

	public void setExtractContent(boolean extractContent) {
		this.extractContent = extractContent;
	}

	public int getPolitenessDelay() {
		return politenessDelay;
	}

	public void setPolitenessDelay(int politenessDelay) {
		this.politenessDelay = politenessDelay;
	}

	public int getMaxNumberOfPagesToFetch() {
		return maxNumberOfPagesToFetch;
	}

	public void setMaxNumberOfPagesToFetch(int maxNumberOfPagesToFetch) {
		this.maxNumberOfPagesToFetch = maxNumberOfPagesToFetch;
	}

	public int getMaxDepthOfCrawling() {
		return maxDepthOfCrawling;
	}

	public void setMaxDepthOfCrawling(int maxDepthOfCrawling) {
		this.maxDepthOfCrawling = maxDepthOfCrawling;
	}

	public boolean isExtractJsonLd() {
		return extractJsonLd;
	}

	public void setExtractJsonLd(boolean extractJsonLd) {
		this.extractJsonLd = extractJsonLd;
	}

	public File getCrawlConfig() {
		return crawlConfig;
	}

	public void setCrawlConfig(File crawlConfig) {
		this.crawlConfig = crawlConfig;
	}

	public File getExtractConfig() {
		return extractConfig;
	}

	public void setExtractConfig(File extractConfig) {
		this.extractConfig = extractConfig;
	}
	
	

}
