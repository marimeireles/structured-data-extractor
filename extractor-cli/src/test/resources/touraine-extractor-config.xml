<?xml version="1.0" encoding="UTF-8"?>
<beans xmlns="http://www.springframework.org/schema/beans"
	     xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
       xmlns:context="http://www.springframework.org/schema/context"
       xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-4.3.xsd
           http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-4.3.xsd
           http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.3.xsd
           http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.3.xsd">
 
	
 	<bean id="propertyConfigurer" class="org.springframework.context.support.PropertySourcesPlaceholderConfigurer">
	  <property name="properties">  
		<props>
			<!-- 
			Path to a file where the crawl seeds are listed, one URL per line.
			The seeds are the URLs at which the crawl will begin.
			-->
			<prop key="seeds.file.path">seeds.txt</prop>
			<!-- 
			Path or URL of the output of the structured data extraction.
			If a file name is given, the output will be stored in memory and then written 
			to this file at the end of the crawl (so make sure to give enough memory to the crawler !).
			If an HTTP URL is given, this is interpreted as the URL of a Sesame RDF repository to
			which the extracted data will be sent.
			-->
			<prop key="output.file.or.repository">http://localhost:7200/repositories/touraine</prop>
			<!--
			Be polite: Make sure that we don't send more than 1 request per
			second (1000 milliseconds between requests). This is in milliseconds.
			-->
		    <prop key="politeness.delay.in.milliseconds">1000</prop>
		    <!--
			The maximum depth of the crawling from the seeds. Set to -1 for unlimited depth.
			-->
		    <prop key="max.depth.of.crawling">-1</prop>
		    <!--
			The maximum number of pages to crawl. The default value is -1 for unlimited number of pages
			-->
		    <prop key="max.pages.to.fetch">20000</prop>
		    <!--
			Path to a folder where intermediate crawl data is stored.
			-->
		    <prop key="crawl.storage.folder">crawl</prop>
		    <!--
			This config parameter can be used to set your crawl to be resumable
			(meaning that you can resume the crawl from a previously
			interrupted/crashed crawl). Note: if you enable resuming feature and
			want to start a fresh crawl, you need to delete the contents of
			rootFolder manually.
			-->
		    <prop key="resumable.crawling">true</prop>
		    <!-- 
		    Indicates if the extractor should process JSON-LD in the page (by default only RDFa is processed).
		    -->
		    <prop key="extract.jsonld">true</prop>
		    <!-- 
		    Indicates if the extractor should process JSON-LD in the page (by default only RDFa is processed).
		    -->
		    <prop key="user.agent">StructuredDataCrawler (http://www.sparna.fr/structureddatacrawler)</prop>
		    <!-- 
		    Indicates if the extractor should attempt to parse the page first as XHTML if RDFa is inserted as RDFa 1.0
		    in a XHTML strict document.
		    -->
		    <prop key="attempt.xhtml.parsing">false</prop>
		</props>  
	  </property>
	  <property name="locations">
	    <list>
	        <value>classpath:extractor-cli.properties</value>
	    </list>
	  </property>
	  <property name="ignoreResourceNotFound" value="true" />
	</bean>



	<!-- ******************************************************************************** -->
	<!-- ********** DON'T MODIFY ANYTHING BELOW THIS LINE - THINGS MAY BREAK ! ********** -->
	<!-- ******************************************************************************** -->



 	<!--  activate @Autowired annotations -->
 	<context:annotation-config />

	<bean id="repositoryFactory" class="fr.sparna.rdf.extractor.cli.crawl.RepositoryFactoryFromString">
		<constructor-arg value="${output.file.or.repository}" />
	</bean>
	<bean id="repository" factory-bean="repositoryFactory" factory-method="newRepository" />

	<bean id="seeds" class="fr.sparna.rdf.extractor.cli.crawl.Seeds">
		<constructor-arg name="path" value="${seeds.file.path}" />
	</bean>

	<bean id="crawlConfig" class="edu.uci.ics.crawler4j.crawler.CrawlConfig">
		<!--
		Be polite: Make sure that we don't send more than 1 request per
		second (1000 milliseconds between requests). This is in milliseconds
		-->
		<property name="politenessDelay" value="${politeness.delay.in.milliseconds}" />
		<!--
		The maximum depth of the crawling from the seeds. Set to -1 for
		unlimited depth.
		-->
		<property name="maxDepthOfCrawling" value="${max.depth.of.crawling}" />
		<!--
		The maximum number of pages to crawl. The default value
		is -1 for unlimited number of pages
		-->
		<property name="maxPagesToFetch" value="${max.pages.to.fetch}" />
		<!--
		Path to a folder where intermediate crawl data is
		stored.
		-->
		<property name="crawlStorageFolder" value="${crawl.storage.folder}" />
		<!--
		Do we want crawler4j to crawl also binary data ?
		example: the contents of pdf, or the metadata of images etc.
		This is not necessary when extracting RDFa metadata from web pages.
		-->
		<property name="includeBinaryContentInCrawling" value="false" />
		<!--
		This config parameter can be used to set your crawl to be resumable
		(meaning that you can resume the crawl from a previously
		interrupted/crashed crawl). Note: if you enable resuming feature and
		want to start a fresh crawl, you need to delete the contents of
		rootFolder manually.
		-->
		<property name="resumableCrawling" value="${resumable.crawling}" />
		<!--
		The User-Agent header that is sent by all requests. Be polite and include
		a link to the project page.
		-->
		<property name="userAgentString" value="${user.agent}" />
	</bean>


 	<bean id="pageFetcher" class="edu.uci.ics.crawler4j.fetcher.PageFetcher">
		<constructor-arg ref="crawlConfig" />
	</bean>

	<bean id="crawlController" class="edu.uci.ics.crawler4j.crawler.CrawlController">
		<constructor-arg name="config" ref="crawlConfig" />
		<constructor-arg name="pageFetcher" ref="pageFetcher" />
		<constructor-arg name="robotstxtServer">
			<bean id="robotstxtServer" class="edu.uci.ics.crawler4j.robotstxt.RobotstxtServer">
				<constructor-arg name="config">
					<bean id="robotstxtConfig" class="edu.uci.ics.crawler4j.robotstxt.RobotstxtConfig">
					</bean>
				</constructor-arg>
				<constructor-arg name="pageFetcher" ref="pageFetcher" />
			</bean>
		</constructor-arg>
	</bean>
 
	<!-- SCOPE: rules for which discovered URIs to crawl; order is very 
	  important because last decision returned other than 'NONE' wins. -->
	<bean id="scope" class="fr.sparna.rdf.extractor.cli.crawl.deciderules.DecideRuleSequence">
		<property name="rules">
			<list>
				<!-- Begin by REJECTing all... -->
				<bean class="fr.sparna.rdf.extractor.cli.crawl.deciderules.RejectDecideRule">
				</bean>
				<!-- ACCEPT only those with domains in the seed list... -->
				<bean class="fr.sparna.rdf.extractor.cli.crawl.deciderules.OnSeedsDomainDecideRule">
					<constructor-arg name="seeds" ref="seeds" />
				</bean>
				<!-- ...and REJECT those from a configurable set of URI regexes... -->
				<bean class="fr.sparna.rdf.extractor.cli.crawl.deciderules.MatchesListRegexDecideRule">
				      <property name="decision" value="REJECT"/>
				      <property name="logicalOr" value="true" />
				      <property name="regexes">
				       <list>
				       	 <value>.*(?i)(\.(js|css))$</value>
				       	 <value>.*(?i)(\.(bmp|gif|jpe?g|png|svg|tiff?))$</value>
				       	 <value>.*(?i)(\.(pdf|doc|xls|odt))$</value>
				       	 <value>.*(?i)(\.(xml))$</value>
				         <value>.*(?i)(\.(avi|wmv|mpe?g|mp3))$</value>
				         <value>.*(?i)(\.(rar|zip|tar|gz))$</value>
				         <value>.*(?i)(\.(txt|conf|pdf))$</value>
				         <value>.*(?i)(\.(swf))$</value>
				       </list>
				      </property>
				</bean>
				<bean class="fr.sparna.rdf.extractor.cli.crawl.deciderules.MatchesListRegexDecideRule">
				      <property name="decision" value="REJECT"/>
				      <property name="logicalOr" value="true" />
				      <property name="regexes">
				       <list>
				       	 <value>http://luynes.fr/ca-bouge-a-luynes/calendrier-des-manifestations.*$</value>
				       	 <value>http://(?!37$).agendaculturel.fr/.**$</value>
				       </list>
				      </property>
				</bean>
			</list>
		</property>
	</bean>

	<bean id="webPageExtractorFactory" class="fr.sparna.rdf.extractor.WebPageExtractorFactory">
		<property name="extractRdfa" value="true" />
		<property name="attemptXhtmlParsing" value="${attempt.xhtml.parsing}" />
		<property name="extractMicrodata" value="true" />
		<property name="extractJsonLd" value="${extract.jsonld}" />
		<property name="extractContent" value="true" />
	</bean>

	<!-- This bean is Autowired with a Repository, a WebPageExtractorFactory and a DecideRule -->
	<bean id="extractorCrawlerFactory" class="fr.sparna.rdf.extractor.cli.crawl.ExtractorCrawlerFactory">
		<property name="graphAware" value="true" />
	</bean>
 
</beans>